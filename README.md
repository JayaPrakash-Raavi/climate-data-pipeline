# ğŸŒ Real-Time Environmental Data Lakehouse for Research and Analytics

> An end-to-end Data Engineering pipeline for ingesting, validating, processing, storing, and visualizing climate and environmental data â€” in real time and batch modes.

## ğŸš€ Overview

This project builds a real-time and batch-enabled pipeline to handle environmental sensor data collected across 22+ locations. The pipeline supports ingestion via Kafka, transformation using Airflow, validation with Great Expectations, storage in MySQL, and exposure via FastAPI & React.js.

> âœ… Built to simulate real-world data engineering workflows  
> âœ… Emphasizes data quality, pipeline orchestration, and API delivery  
> âœ… Deployed using Docker and optionally AWS

## ğŸ”§ Tech Stack

| Layer              | Tool/Service             |
|-------------------|--------------------------|
| **Streaming**      | Apache Kafka             |
| **Processing**     | Apache Airflow, Python   |
| **Validation**     | Great Expectations       |
| **Storage**        | MySQL, AWS S3 (optional) |
| **API**            | FastAPI                  |
| **Frontend**       | React.js                 |
| **Monitoring**     | Prometheus, Grafana      |
| **Containerization** | Docker, Docker Compose  |
| **CI/CD**          | GitHub Actions           |

## ğŸ§± Architecture

```
[Sensors / CSV Files] â†’ Kafka Producer â†’ Kafka Topic (Raw Data)
                                     â†“
                             Kafka Consumer (validation & transformation)
                                     â†“
                            Staging Database (MySQL/PostgreSQL)
                                     â†“
                      Airflow DAG â†’ Processed Data Warehouse
                                     â†“
                     FastAPI REST API â†’ React Dashboard
                                     â†“
                            Researchers / Analysts
```

## ğŸ“‚ Project Structure

```
climate-data-pipeline/
â”œâ”€â”€ airflow/                # Airflow DAGs
â”œâ”€â”€ api/                    # FastAPI backend
â”œâ”€â”€ consumer.py             # Kafka Consumer
â”œâ”€â”€ producer.py             # Kafka Producer
â”œâ”€â”€ data/                   # Sample CSV files
â”œâ”€â”€ frontend/               # React.js app
â”œâ”€â”€ expectations/           # Great Expectations suite
â”œâ”€â”€ docker-compose.yml      # Services definition
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md               # Project documentation
â””â”€â”€ .github/workflows/      # CI/CD with GitHub Actions
```

## ğŸ—‚ï¸ Data Sources

- Simulated environmental sensor data (temperature, humidity, SWE, radiation, etc.)
- Historical `.csv` files used to simulate real-time streaming
- 22 unique location IDs

## ğŸ” Pipeline Components

### 1. Kafka Producer

- Simulates streaming by reading `.csv` files line by line
- Publishes messages to the Kafka topic `climate_data_raw`

### 2. Kafka Consumer

- Listens to `climate_data_raw`
- Validates and transforms each message
- Writes cleaned records into staging MySQL table

### 3. Airflow DAG

- Scheduled batch ETL pipeline
- Cleans & loads data from staging â†’ processed schema
- Integrates Great Expectations validation

### 4. FastAPI

- RESTful API to access processed data
- Supports:
  - Query by location & time
  - Data export
  - Attribute listing

### 5. React Dashboard

- Displays interactive charts for researchers
- Filters by:
  - Location
  - Timestamp range
  - Attribute type
- Data export to CSV supported

## âœ… Data Validation

Validation rules follow QA/QC protocols:

| Attribute         | Validation Rule |
|------------------|------------------|
| Air Temperature  | -35Â°C to 50Â°C    |
| RH               | 3% to 103%       |
| SWE              | â‰¥ 0              |
| SW/LW Radiation  | -0.4 to 1500     |
| Wind Speed       | 0 to 40 m/s      |
| ...              | Temporal rules, range checks, etc. |

> Real-time validation in Kafka Consumer  
> Batch validation via Great Expectations in Airflow

## âš™ï¸ Local Setup Instructions

### Prerequisites

- Docker & Docker Compose
- Python 3.9+
- Node.js & npm (for frontend)

### 1. Clone and Configure

```bash
git clone https://github.com/your-username/climate-data-pipeline.git
cd climate-data-pipeline
```

### 2. Start Services

```bash
# Start Kafka, Zookeeper, MySQL, Airflow
docker-compose up -d
```

### 3. Run Producer and Consumer

```bash
# Produce data
python producer.py --input data/sample_location.csv --topic climate_data_raw

# Consume data
python consumer.py --topic climate_data_raw
```

### 4. Launch API & Dashboard

```bash
# API
cd api
uvicorn main:app --reload

# Frontend
cd ../frontend
npm install
npm start
```

## ğŸ“Š Example API Usage

| Endpoint         | Method | Description              |
|------------------|--------|--------------------------|
| `/api/data`      | GET    | Query processed records  |
| `/api/export`    | GET    | Export to CSV            |
| `/api/locations` | GET    | List available locations |
| `/api/fields`    | GET    | List supported attributes|

## ğŸ“ˆ Monitoring (Optional)

- Kafka â†’ Prometheus Kafka Exporter
- Airflow â†’ Built-in + Prometheus
- MySQL â†’ Grafana dashboard
- API â†’ FastAPI metrics middleware

## ğŸ“Œ Roadmap

- [ ] Schema registry with Avro  
- [ ] Anomaly detection in real-time  
- [ ] Snowflake or BigQuery integration  
- [ ] Auto-scaling via Kubernetes  
- [ ] Data cataloging with OpenMetadata  

## ğŸ“œ License

MIT License. Feel free to fork, modify, and build upon it.

## ğŸ™Œ Acknowledgments

- Inspired by real-world research challenges at UVM  
- Kafka, FastAPI, and Airflow documentation  
- Great Expectations open-source contributors  

## ğŸ‘¤ Author

**Jaya Prakash Narayana Raavi**  
Data Engineer & Full Stack Developer  
ğŸ”— [LinkedIn](https://linkedin.com/in/your-profile)  
ğŸ“§ your.email@example.com  


